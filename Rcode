#****載入套件****
library(tidyverse)
library(rvest) 
library(polite)
library(jiebaR)
library(stopwords)
library(pdftools)
library(tm)
library(here)
library(tibble)
library(ggplot2)
library(ggthemes)
library(ldatuning)
library(stm) 
library(LDAvis)
#****爬蟲****
website_freedom <- "https://news.ltn.com.tw/news/world/breakingnews/4095188"
website_unite <-"https://udn.com/news/story/123078/6694989"
doc <- read_html(website_freedom, encoding = "UTF-8")
scraped_text_freedom <- 
  # 挑選網頁中要擷取的資料位置
  rvest::html_elements(doc,css = "p:nth-child(4) ,.after_ir+ p , .time+ p,.after_ir~ p+ p , p:nth-child(9)") %>% 
  # 將該位置的文字資料讀出
  rvest::html_text()
scraped_dataframe_freedom <-
  tibble(scraped_text_freedom)
doc1 <- read_html(website_unite, encoding = "UTF-8")
scraped_text_unit <- 
  # 挑選網頁中要擷取的資料位置
  rvest::html_elements(doc1,css = ".article-content__editor") %>% 
  # 將該位置的文字資料讀出
  rvest::html_text()
scraped_dataframe_unit <-
  tibble(scraped_text_unit)
#****斷詞****

seg_engine <- jiebaR::worker()
seg_engine
seg_engine_bylines <- jiebaR::worker(bylines = TRUE) # 斷詞的結果會變成list的格式
jiebaR::new_user_word(seg_engine_bylines, words = 
                        c("栗戰書", "韓正歲",'丁薛祥','陳敏爾','黃坤明', '李希','王滬寧','蔡奇','趙樂際','李強','習近平','胡春華'))
#****前處理****
segment_rmnum <- scraped_dataframe_unit %>%
  mutate(words =  jiebaR::segment(scraped_text_unit, 
                                  jiebar = seg_engine_bylines)) %>%
  tidyr::unnest(words)

segment_rmnum <- segment_rmnum %>% 
  mutate(words = str_replace_all(words, pattern = "[0-9]+", replacement = "")) %>%                # <- 同學題目應該沒有要執行這幾行....
  mutate(words=str_replace_all(words, pattern = "[a-z]+",  replacement = "" )) %>%                # <- 
  mutate(words=str_replace_all(words, pattern = "[A-Z]+",  replacement = "" )) %>%                # <-
  mutate(words=str_replace_all(words, pattern = "[A-Z][a-z]+",  replacement = "" )) %>% 
  filter(words!="") %>%
  filter(!str_detect(words, pattern = "\\d"), 
         str_length(words) >= 2) 
#****停用字****
rm_stopwords <- tibble(words = c(stopwords::stopwords(language = "zh", source = "misc"),"年", "月", "日", "民國", "與", "於", "並", "為", "項"))

rmnum_clear <- segment_rmnum %>% 
  anti_join(rm_stopwords, by = "words") %>% 
  select(words)

segment_freedom <- scraped_dataframe_freedom %>%
  mutate(words =  jiebaR::segment(scraped_text_freedom, 
                                  jiebar = seg_engine_bylines)) %>%
  tidyr::unnest(words)
  rm_stopwords <- tibble(words = c(stopwords::stopwords(language = "zh", source = "misc"),"年", "月", "日", "民國", "與", "於", "並", "為", "項"))

freedom_clear <- segment_freedom %>% 
  anti_join(rm_stopwords, by = "words") %>% 
  select(words)



#自由時報爬蟲 

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest) # 爬取資料用
library(polite) # install.packages("polite")
library(glue) # 文字資料處理，進階版的paste # 應該安裝好tidyverse就已經包含其中了

##爬自由"二十大"9/3~10/22

pages <- 1:11
full_index <- glue::glue("https://search.ltn.com.tw/list?keyword=%E4%BA%8C%E5%8D%81%E5%A4%A7&start_time=20220903&end_time=20221022&sort=date&type=all&page={pages}")
#path放入read_html讀取，只有讀取一個頁面，現在有3個頁面要讀取。當然一樣可以使用for loop的方式來達成。

css_title <- ".tit"
css_date <- ".cont .time"
css_link <- ".http"
css_fulltext <- ".text"

# 先製作一個空的tibble object
data_all <- NULL

# 開始一頁一頁爬取資料
for(page in full_index){
  base <- xml2::read_html(page)
  # 將各自要抓取的資料存成vector，再整併成data frame
  title <- base %>% html_elements(css = css_title) %>% 
    html_text() %>% 
    str_squish()
  date <- base %>% html_elements(css = css_date) %>% 
    html_text()
  link <- base %>% html_elements(css = css_link) %>% 
    html_attr(name = "href") %>% 
    str_squish()
  
  data_this <- tibble(title = title, date = date, link = link) #
  
  data_all <- bind_rows(data_all, data_this)
  Sys.sleep(time = runif(n = 1, min = 1, max = 2))
}

# 再利用剛剛取得的內文連結爬取全文。
fulltext <- vector(mode = "character", length = nrow(data_all))

for(i in seq_along(data_all$link)){
  fulltext[i] <- read_html(data_all$link[i]) %>% 
    html_elements(css = css_fulltext) %>% 
    html_text() %>% 
    str_squish()
  Sys.sleep(time = runif(n = 1, min = 1, max = 2))
}
beepr::beep(2)

# 併入原先的data_all，成為新的一欄
data_all <- data_all %>% mutate(fulltext = fulltext)

# View(data_all)


###############################################
##爬自由02"二十大"10/23~12/11


pages_02 <- 1:15
full_index_02 <- glue::glue("https://search.ltn.com.tw/list?keyword=%E4%BA%8C%E5%8D%81%E5%A4%A7&start_time=20221023&end_time=20221211&sort=date&type=all&page={pages_02}")

css_title_02 <- ".tit"
css_date_02 <- ".cont .time"
css_link_02 <- ".http"
css_fulltext_02 <- ".text"

data_all_02 <- NULL

# 開始一頁一頁爬取資料
for(page in full_index_02){
  base_02 <- xml2::read_html(page)
  # 將各自要抓取的資料存成vector，再整併成data frame
  title_02 <- base_02 %>% html_elements(css = css_title_02) %>% 
    html_text() %>% 
    str_squish()
  date_02 <- base_02 %>% html_elements(css = css_date_02) %>% 
    html_text()
  link_02 <- base_02 %>% html_elements(css = css_link_02) %>% 
    html_attr(name = "href") %>% 
    str_squish()
  
  data_this_02 <- tibble(title = title_02, date = date_02, link = link_02) 
  
  data_all_02 <- bind_rows(data_all_02, data_this_02)
  Sys.sleep(time = runif(n = 1, min = 1, max = 2))
}

# 再利用剛剛取得的內文連結爬取全文。
fulltext_02 <- vector(mode = "character", length = nrow(data_all_02))

for(i in seq_along(data_all_02$link)){
  fulltext_02[i] <- read_html(data_all_02$link[i]) %>% 
    html_elements(css = css_fulltext_02) %>% 
    html_text() %>% 
    str_squish()
  Sys.sleep(time = runif(n = 1, min = 1, max = 2))
}
beepr::beep(2)

# 併入原先的data_all，成為新的一欄
data_all_02 <- data_all_02 %>% mutate(fulltext = fulltext_02)
